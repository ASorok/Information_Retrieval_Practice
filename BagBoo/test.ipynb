{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import random as r\n",
    "import sys\n",
    "\n",
    "def MSE(x, y):\n",
    "    return ((x - y)**2).sum() / float(len(x))\n",
    "\n",
    "#calculate variance of set\n",
    "def variance_calculate(X):\n",
    "#     n_points = X.shape[0]\n",
    "#     var = 0\n",
    "    \n",
    "#     for i1 in xrange(n_points):\n",
    "#         for i2 in xrange(i1 + 1, n_points):\n",
    "#             var += (X[i1] - X[i2]) ** 2;\n",
    "#             print i1, i2, var\n",
    "    \n",
    "#     print \"calculate varianse of \", X, (1.0 / (2 * n_points ** 2)) * var\n",
    "    \n",
    "#     return (1.0 / n_points ** 2) * var\n",
    "#     print X\n",
    "    return np.var(X)\n",
    "    \n",
    "# search best split in X of split_feature\n",
    "def search_best_split_of_feat(X, Y, split_feature, orig_var, min_samples_leaf):\n",
    "    best_gain = 0\n",
    "    checked_split_value = []\n",
    "    best_split_value = None\n",
    "    res_X_l = None\n",
    "    res_X_r = None\n",
    "    res_Y_l = None\n",
    "    res_Y_r = None\n",
    "\n",
    "    last_X_l = None\n",
    "    last_X_r = None\n",
    "    last_split_value = 0\n",
    "    \n",
    "    for split_value in X[:, split_feature]:\n",
    "        if split_value in checked_split_value:\n",
    "            continue\n",
    "\n",
    "        checked_split_value.append(split_value)\n",
    "        if last_X_l is None:\n",
    "#         print np.where(X[:, split_feature] < split_value)\n",
    "            X_l = X[np.where(X[:, split_feature] < split_value)]\n",
    "            X_r = X[np.where(X[:, split_feature] >= split_value)]\n",
    "            Y_l = Y[np.where(X[:, split_feature] < split_value)]\n",
    "            Y_r = Y[np.where(X[:, split_feature] >= split_value)]\n",
    "\n",
    "            last_X_l = np.where(X[:, split_feature] < split_value)\n",
    "            last_X_r = np.where(X[:, split_feature] >= split_value)\n",
    "            last_split_value = split_value\n",
    "\n",
    "        else:\n",
    "            if split_value >= last_split_value:\n",
    "                X_l = X[last_X_l]\n",
    "                Y_l = Y[last_X_l]\n",
    "                new_X_l = []\n",
    "                new_X_r = []\n",
    "                for idx in last_X_r:\n",
    "                    if X[idx, split_feature] < split_value:\n",
    "                        new_X_l.append(idx)\n",
    "                    else:\n",
    "                        new_X_r.append(idx)\n",
    "                X_l.append(X[new_X_l], axis = 0)\n",
    "                Y_l.append(X[new_X_l], axis = 0)\n",
    "\n",
    "                X_r = X[new_X_r]\n",
    "                Y_r = Y[new_X_r]\n",
    "\n",
    "                last_X_l = last_X_l.append(new_X_l)\n",
    "                last_X_r = new_X_r\n",
    "                last_split_value = split_value\n",
    "            else:\n",
    "                X_r = X[last_X_r]\n",
    "                Y_r = Y[last_X_r]\n",
    "                new_X_l = []\n",
    "                new_X_r = []\n",
    "                for idx in last_X_r:\n",
    "                    if X[idx, split_feature] >= split_value:\n",
    "                        new_X_r.append(idx)\n",
    "                    else:\n",
    "                        new_X_l.append(idx)\n",
    "                X_r.append(X[new_X_r], axis = 0)\n",
    "                Y_r.append(X[new_X_r], axis = 0)\n",
    "\n",
    "                X_l = X[new_X_l]\n",
    "                Y_l = Y[new_X_l]\n",
    "\n",
    "                last_X_l = last_X_l.append(new_X_l)\n",
    "                last_X_r = new_X_r\n",
    "                last_split_value = split_value\n",
    "#         print \"split_value: \", split_value\n",
    "#         print X_l, X_r, Y_l, Y_r\n",
    "#         print X_l.shape[0]\n",
    "#         print X_r.shape[0]\n",
    "        if X_l.shape[0] >= min_samples_leaf and X_r.shape[0] > min_samples_leaf:\n",
    "            var_l = variance_calculate(Y_l)\n",
    "            var_r = variance_calculate(Y_r)\n",
    "            \n",
    "#             print orig_var, var_l, var_r,\\\n",
    "#                 (orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r), best_gain\n",
    "#             if (orig_var - var_l - var_r) > best_gain:\n",
    "            if (orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r) > best_gain:\n",
    "                best_gain = orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r\n",
    "                best_split_value = split_value\n",
    "                res_X_l = X_l\n",
    "                res_X_r = X_r\n",
    "                res_Y_l = Y_l\n",
    "                res_Y_r = Y_r\n",
    "    \n",
    "    return best_gain, best_split_value, res_X_l, res_X_r, res_Y_l, res_Y_r\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self, split_feature = None, split_val = None,\\\n",
    "                 left = None, right = None, n_points = None, depth = 10, min_samples_leaf = 1):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_val = split_val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.data = None\n",
    "        self.n_points = n_points\n",
    "        self.depth = depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    \n",
    "    #return MSE\n",
    "    def score(self, X, Y):\n",
    "        predicted = self.predict(X)\n",
    "        MSE = ((predicted - Y)**2).sum() / float(len(X))\n",
    "        print \"MSE: \", MSE\n",
    "\n",
    "    def predict_one(self, sample):\n",
    "        if self.data is not None:\n",
    "            return self.data\n",
    "\n",
    "        if sample[self.split_feature] >= self.split_value:\n",
    "            return self.right.predict_one(sample)\n",
    "        else:\n",
    "            return self.left.predict_one(sample)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for sample in X:\n",
    "#             print sample\n",
    "            predicted.append(self.predict_one(sample))\n",
    "        return np.asarray(predicted)\n",
    "            \n",
    "    #building tree\n",
    "    def fit (self, X, Y):\n",
    "#         print \"start_fitting:\", X, Y\n",
    "        self.n_points = X.shape[0]\n",
    "        if self.n_points <= self.min_samples_leaf or self.depth == 0:\n",
    "#             print \"N_POINTS: \", self.n_points\n",
    "            self.data = np.mean(Y)\n",
    "            return self\n",
    "        \n",
    "        best_gain = 0\n",
    "        res_split_value = 0\n",
    "        res_split_feature = None\n",
    "        res_X_r = None\n",
    "        res_X_l = None\n",
    "        res_Y_l = None\n",
    "        res_Y_r = None\n",
    "        orig_var = variance_calculate(Y)\n",
    "        \n",
    "        n_feat = X.shape[1]\n",
    "        for feat in xrange(n_feat):\n",
    "#             print \n",
    "#             print \"Analusys of feature \", feat\n",
    "            gain, split_value, X_l , X_r, Y_l, Y_r = search_best_split_of_feat(X, Y, feat, orig_var, self.min_samples_leaf)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                res_split_value = split_value\n",
    "                res_split_feature = feat\n",
    "                res_X_r = X_r\n",
    "                res_X_l = X_l\n",
    "                res_Y_l = Y_l\n",
    "                res_Y_r = Y_r\n",
    "        \n",
    "#         print \"BEST SPLIT: \"\n",
    "#         print res_split_feature, res_split_value\n",
    "#         print res_X_l, res_X_r, res_Y_l, res_Y_r\n",
    "#         print \n",
    "            \n",
    "        if best_gain > 0:\n",
    "            self.split_feature = res_split_feature\n",
    "            self.split_value = res_split_value\n",
    "            if self.depth is None:\n",
    "                self.left = Tree(min_points_leaf = self.min_points_leaf).fit(res_X_l, res_Y_l)\n",
    "                self.right = Tree(min_points_leaf = self.min_points_leaf).fit(res_X_r, res_Y_r)\n",
    "            else:\n",
    "                self.left = Tree(depth = self.depth - 1, min_samples_leaf = self.min_samples_leaf).fit(res_X_l, res_Y_l)\n",
    "                self.right = Tree(depth = self.depth - 1, min_samples_leaf = self.min_samples_leaf).fit(res_X_r, res_Y_r)\n",
    "                \n",
    "        else:\n",
    "            self.n_points = len(Y)\n",
    "#             print \"N_POINTS: \", self.n_points\n",
    "#             print X\n",
    "#             print Y\n",
    "            self.data = np.mean(Y)\n",
    "        return self\n",
    "            \n",
    "    def print_tree(self):\n",
    "        if self.data != None:\n",
    "            print str(self.data)\n",
    "        else:\n",
    "            print str(self.split_feature) + \": \" + str(self.split_value) + \"?\"\n",
    "            print \"\\t T->\"\n",
    "            if (self.right):\n",
    "                self.right.print_tree()\n",
    "            print \"\\t F->\"\n",
    "            if self.left:\n",
    "                self.left.print_tree()\n",
    "        \n",
    "\n",
    "class Gradient_Boosting:\n",
    "    def __init__(self, n_estimators=10, shrinkage=0.05, max_depth=10, min_samples_leaf=1):\n",
    "        self.estimators_list = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.shrinkage = shrinkage\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.estimators_list = []\n",
    "        first_estimator = Tree(depth= self.max_depth, min_samples_leaf= self.min_samples_leaf).fit(X, Y)\n",
    "        self.estimators_list.append(first_estimator)\n",
    "        \n",
    "        current_predict = first_estimator.predict(X)\n",
    "#         sys.stderr.write('\\rLearning estimator number: 0'+ \"/\" + str(self.n_estimators))\n",
    "#         print  \"\\tLearning estimator number: 0 ; MSE error on train dataset: \", MSE(current_predict, Y)\n",
    "        \n",
    "        for i in xrange(1, self.n_estimators):\n",
    "            \n",
    "            antigrad = Y - current_predict\n",
    "            \n",
    "            new_estimator = Tree(depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n",
    "            new_estimator = new_estimator.fit(X, antigrad)\n",
    "#             new_estimator.print_tree()\n",
    "            \n",
    "#             print set(antigrad)\n",
    "#             print new_estimator.predict(X)[:10]\n",
    "            current_predict += self.shrinkage * new_estimator.predict(X)\n",
    "            \n",
    "#             if i % 10 == 0:\n",
    "#                 print \"\\tLearning estimator number: \", i,\\\n",
    "#                         \"; MSE error on train dataset: \", MSE(current_predict, Y)\n",
    "            \n",
    "            sys.stderr.write('\\rLearning estimator number: '+ str(i)+\"/\" + str(self.n_estimators) \\\n",
    "                             + \"; MSE error on train dataset: \" + str(MSE(current_predict, Y)))\n",
    "            self.estimators_list.append(new_estimator)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y =  self.estimators_list[0].predict(X)\n",
    "#         print len(y)\n",
    "        for estimator in self.estimators_list[1:]:\n",
    "            y += estimator.predict(X) * self.shrinkage\n",
    "#             print MSE(y, Y)\n",
    "        return y\n",
    "\n",
    "#n_bag -count of bagging iteration\n",
    "#n_boo - count of tree in Gradien Boosting\n",
    "#max_depth - max depth of trees TODO:dinamic depth of trees\n",
    "#min_samples_leaf\n",
    "#bagging_ratio -cnt of samples (in percent), which using for bagging interation\n",
    "#RSM and Bagging without replacement\n",
    "class BagBoo:\n",
    "    def __init__ (self, n_boo = 10, n_bag = 10, bagging_ratio = 0.1, rsm_ratio = 1, max_depth = 10,\\\n",
    "                  min_samples_leaf = 1, shrinkage = 0.1):\n",
    "        self.n_boo = n_boo\n",
    "        self.n_bag = n_bag\n",
    "        self.bagging_ratio = bagging_ratio\n",
    "        self.rsm_ratio = rsm_ratio\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boosting_list = []\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def fit(self, X, Y, verbose = 0, X_test = None, Y_test = None):\n",
    "        cur_sum_predict = 0 \n",
    "        cur_sum_train = 0\n",
    "        error_statistic = []\n",
    "        rsm_cnt = int(self.rsm_ratio * X.shape[1])\n",
    "        bagging_cnt = int(self.bagging_ratio * X.shape[0])\n",
    "        fittint_samples = set()\n",
    "        print \"features in RSM: \", rsm_cnt\n",
    "        print \"samples in bagging: \", bagging_cnt\n",
    "        for bag_iter in xrange(self.n_bag):\n",
    "            sys.stderr.write('\\rIteration of bagging:'+str(bag_iter) + \"/\" + str(self.n_bag))\n",
    "#            print \"Iteration of bagging: \"+ str(bag_iter) + \"/\" + str(self.n_bag)\n",
    "            shuffle_idx_bagging = range(X.shape[0])\n",
    "            shuffle_idx_rsm = range(X.shape[1])\n",
    "            \n",
    "            r.shuffle(shuffle_idx_bagging)\n",
    "            r.shuffle(shuffle_idx_rsm)\n",
    "            shuffle_idx_bagging = shuffle_idx_bagging[:bagging_cnt]\n",
    "            shuffle_idx_rsm = shuffle_idx_rsm[:rsm_cnt]\n",
    "            fittint_samples.update(shuffle_idx_bagging)\n",
    "#             print shuffle_idx_bagging, shuffle_idx_rsm\n",
    "            X_bag = X[shuffle_idx_bagging][:, shuffle_idx_rsm]\n",
    "            Y_bag = Y[shuffle_idx_bagging]\n",
    "#             print \"sgs\", X_bag, Y_bag\n",
    "            \n",
    "            new_boosting = Gradient_Boosting(n_estimators= self.n_boo, max_depth=self.max_depth,\\\n",
    "                                             min_samples_leaf= self.min_samples_leaf, shrinkage = self.shrinkage)\n",
    "            new_boosting.fit(X_bag, Y_bag)\n",
    "            \n",
    "            self.boosting_list.append(new_boosting)\n",
    "            \n",
    "            print \"Iteration of Bagging:\", bag_iter, \"/\", self.n_bag\n",
    "            if verbose:\n",
    "                print \"fitting_samples: \", len(fittint_samples) / float(X.shape[0])\n",
    "                cur_sum_predict += new_boosting.predict(X_test)\n",
    "                cur_sum_train += new_boosting.predict(X)\n",
    "                error_test =  MSE(cur_sum_predict / float(len(self.boosting_list)), Y_test)\n",
    "                error_train =  MSE(cur_sum_train / float(len(self.boosting_list)), Y)\n",
    "                print \"MSE on test Dataset:\", error_test, \"Iteration of Bagging:\", bag_iter, \"/\", self.n_bag\n",
    "                print \"MSE on train Dataset:\", error_train, \"Iteration of Bagging:\", bag_iter, \"/\", self.n_bag\n",
    "                error_statistic.append(error_test)\n",
    "               \n",
    "        return error_statistic    \n",
    "\n",
    "    def predict(self, X):\n",
    "        y = np.array([0.0] * X.shape[0])\n",
    "        for boosting in self.boosting_list:\n",
    "            y += boosting.predict(X)\n",
    "        return y / float(self.n_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 5], [2, 3, 5], [2, 3, 4], [2, 9, 1], [2, 9, 4], [1, 2, 1]])\n",
    "Y = np.array([1, 0, 3, 6, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-de669f7c4b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-eb081387c464>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m#             print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m#             print \"Analusys of feature \", feat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_l\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_best_split_of_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mbest_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-eb081387c464>\u001b[0m in \u001b[0;36msearch_best_split_of_feat\u001b[0;34m(X, Y, split_feature, orig_var, min_samples_leaf)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mnew_X_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlast_X_r\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                         \u001b[0mnew_X_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "tree = Tree().fit(X, Y)\n",
    "tree.print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
