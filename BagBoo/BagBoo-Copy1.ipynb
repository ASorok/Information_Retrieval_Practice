{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BagBoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import random as r\n",
    "import sys\n",
    "\n",
    "def MSE(x, y):\n",
    "    return ((x - y)**2).sum() / float(len(x))\n",
    "\n",
    "#calculate variance of set\n",
    "def variance_calculate(X):\n",
    "#     n_points = X.shape[0]\n",
    "#     var = 0\n",
    "    \n",
    "#     for i1 in xrange(n_points):\n",
    "#         for i2 in xrange(i1 + 1, n_points):\n",
    "#             var += (X[i1] - X[i2]) ** 2;\n",
    "#             print i1, i2, var\n",
    "    \n",
    "#     print \"calculate varianse of \", X, (1.0 / (2 * n_points ** 2)) * var\n",
    "    \n",
    "#     return (1.0 / n_points ** 2) * var\n",
    "#     print X\n",
    "    return np.var(X)\n",
    "    \n",
    "# search best split in X of split_feature\n",
    "def search_best_split_of_feat(X, Y, split_feature, orig_var, min_samples_leaf):\n",
    "    best_gain = 0\n",
    "    checked_split_value = []\n",
    "    best_split_value = None\n",
    "    res_X_l = None\n",
    "    res_X_r = None\n",
    "    res_Y_l = None\n",
    "    res_Y_r = None\n",
    "    \n",
    "    for split_value in X[:, split_feature]:\n",
    "        if split_value in checked_split_value:\n",
    "            continue\n",
    "        checked_split_value.append(split_value)\n",
    "#         print np.where(X[:, split_feature] < split_value)\n",
    "        X_l = X[np.where(X[:, split_feature] < split_value)]\n",
    "        X_r = X[np.where(X[:, split_feature] >= split_value)]\n",
    "        Y_l = Y[np.where(X[:, split_feature] < split_value)]\n",
    "        Y_r = Y[np.where(X[:, split_feature] >= split_value)]\n",
    "#         print \"split_value: \", split_value\n",
    "#         print X_l, X_r, Y_l, Y_r\n",
    "#         print X_l.shape[0]\n",
    "#         print X_r.shape[0]\n",
    "        if X_l.shape[0] >= min_samples_leaf and X_r.shape[0] > min_samples_leaf:\n",
    "            var_l = variance_calculate(Y_l)\n",
    "            var_r = variance_calculate(Y_r)\n",
    "            \n",
    "#             print orig_var, var_l, var_r,\\\n",
    "#                 (orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r), best_gain\n",
    "#             if (orig_var - var_l - var_r) > best_gain:\n",
    "            if (orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r) > best_gain:\n",
    "                best_gain = orig_var - (len(Y_l) / float(len(Y))) * var_l - (len(Y_r) / float(len(Y))) * var_r\n",
    "                best_split_value = split_value\n",
    "                res_X_l = X_l\n",
    "                res_X_r = X_r\n",
    "                res_Y_l = Y_l\n",
    "                res_Y_r = Y_r\n",
    "    \n",
    "    return best_gain, best_split_value, res_X_l, res_X_r, res_Y_l, res_Y_r\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self, split_feature = None, split_val = None,\\\n",
    "                 left = None, right = None, n_points = None, depth = 10, min_samples_leaf = 1):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_val = split_val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.data = None\n",
    "        self.n_points = n_points\n",
    "        self.depth = depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "    \n",
    "    #return MSE\n",
    "    def score(self, X, Y):\n",
    "        predicted = self.predict(X)\n",
    "        MSE = ((predicted - Y)**2).sum() / float(len(X))\n",
    "        print \"MSE: \", MSE\n",
    "\n",
    "    def predict_one(self, sample):\n",
    "        if self.data is not None:\n",
    "            return self.data\n",
    "\n",
    "        if sample[self.split_feature] >= self.split_value:\n",
    "            return self.right.predict_one(sample)\n",
    "        else:\n",
    "            return self.left.predict_one(sample)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for sample in X:\n",
    "#             print sample\n",
    "            predicted.append(self.predict_one(sample))\n",
    "        return np.asarray(predicted)\n",
    "            \n",
    "    #building tree\n",
    "    def fit (self, X, Y):\n",
    "#         print \"start_fitting:\", X, Y\n",
    "        self.n_points = X.shape[0]\n",
    "        if self.n_points <= self.min_samples_leaf or self.depth == 0:\n",
    "#             print \"N_POINTS: \", self.n_points\n",
    "            self.data = np.mean(Y)\n",
    "            return self\n",
    "        \n",
    "        best_gain = 0\n",
    "        res_split_value = 0\n",
    "        res_split_feature = None\n",
    "        res_X_r = None\n",
    "        res_X_l = None\n",
    "        res_Y_l = None\n",
    "        res_Y_r = None\n",
    "        orig_var = variance_calculate(Y)\n",
    "        \n",
    "        n_feat = X.shape[1]\n",
    "        for feat in xrange(n_feat):\n",
    "#             print \n",
    "#             print \"Analusys of feature \", feat\n",
    "            gain, split_value, X_l , X_r, Y_l, Y_r = search_best_split_of_feat(X, Y, feat, orig_var, self.min_samples_leaf)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                res_split_value = split_value\n",
    "                res_split_feature = feat\n",
    "                res_X_r = X_r\n",
    "                res_X_l = X_l\n",
    "                res_Y_l = Y_l\n",
    "                res_Y_r = Y_r\n",
    "        \n",
    "#         print \"BEST SPLIT: \"\n",
    "#         print res_split_feature, res_split_value\n",
    "#         print res_X_l, res_X_r, res_Y_l, res_Y_r\n",
    "#         print \n",
    "            \n",
    "        if best_gain > 0:\n",
    "            self.split_feature = res_split_feature\n",
    "            self.split_value = res_split_value\n",
    "            if self.depth is None:\n",
    "                self.left = Tree(min_points_leaf = self.min_points_leaf).fit(res_X_l, res_Y_l)\n",
    "                self.right = Tree(min_points_leaf = self.min_points_leaf).fit(res_X_r, res_Y_r)\n",
    "            else:\n",
    "                self.left = Tree(depth = self.depth - 1, min_samples_leaf = self.min_samples_leaf).fit(res_X_l, res_Y_l)\n",
    "                self.right = Tree(depth = self.depth - 1, min_samples_leaf = self.min_samples_leaf).fit(res_X_r, res_Y_r)\n",
    "                \n",
    "        else:\n",
    "            self.n_points = len(Y)\n",
    "#             print \"N_POINTS: \", self.n_points\n",
    "#             print X\n",
    "#             print Y\n",
    "            self.data = np.mean(Y)\n",
    "        return self\n",
    "            \n",
    "    def print_tree(self):\n",
    "        if self.data != None:\n",
    "            print str(self.data)\n",
    "        else:\n",
    "            print str(self.split_feature) + \": \" + str(self.split_value) + \"?\"\n",
    "            print \"\\t T->\"\n",
    "            if (self.right):\n",
    "                self.right.print_tree()\n",
    "            print \"\\t F->\"\n",
    "            if self.left:\n",
    "                self.left.print_tree()\n",
    "        \n",
    "\n",
    "class Gradient_Boosting:\n",
    "    def __init__(self, n_estimators=10, shrinkage=0.05, max_depth=10, min_samples_leaf=1):\n",
    "        self.estimators_list = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.shrinkage = shrinkage\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.estimators_list = []\n",
    "        first_estimator = Tree(depth= self.max_depth, min_samples_leaf= self.min_samples_leaf).fit(X, Y)\n",
    "        self.estimators_list.append(first_estimator)\n",
    "        \n",
    "        current_predict = first_estimator.predict(X)\n",
    "#         sys.stderr.write('\\rLearning estimator number: 0'+ \"/\" + str(self.n_estimators))\n",
    "#         print  \"\\tLearning estimator number: 0 ; MSE error on train dataset: \", MSE(current_predict, Y)\n",
    "        \n",
    "        for i in xrange(1, self.n_estimators):\n",
    "            \n",
    "            antigrad = Y - current_predict\n",
    "            \n",
    "            new_estimator = Tree(depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n",
    "            new_estimator = new_estimator.fit(X, antigrad)\n",
    "#             new_estimator.print_tree()\n",
    "            \n",
    "#             print set(antigrad)\n",
    "#             print new_estimator.predict(X)[:10]\n",
    "            current_predict += self.shrinkage * new_estimator.predict(X)\n",
    "            \n",
    "#             if i % 10 == 0:\n",
    "#                 print \"\\tLearning estimator number: \", i,\\\n",
    "#                         \"; MSE error on train dataset: \", MSE(current_predict, Y)\n",
    "            \n",
    "            sys.stderr.write('\\rLearning estimator number: '+ str(i)+\"/\" + str(self.n_estimators) \\\n",
    "                             + \"; MSE error on train dataset: \" + str(MSE(current_predict, Y)))\n",
    "            self.estimators_list.append(new_estimator)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y =  self.estimators_list[0].predict(X)\n",
    "#         print len(y)\n",
    "        for estimator in self.estimators_list[1:]:\n",
    "            y += estimator.predict(X) * self.shrinkage\n",
    "#             print MSE(y, Y)\n",
    "        return y\n",
    "\n",
    "#n_bag -count of bagging iteration\n",
    "#n_boo - count of tree in Gradien Boosting\n",
    "#max_depth - max depth of trees TODO:dinamic depth of trees\n",
    "#min_samples_leaf\n",
    "#bagging_ratio -cnt of samples (in percent), which using for bagging interation\n",
    "#RSM and Bagging without replacement\n",
    "class BagBoo:\n",
    "    def __init__ (self, n_boo = 10, n_bag = 10, bagging_ratio = 0.1, rsm_ratio = 1, max_depth = 10,\\\n",
    "                  min_samples_leaf = 1, shrinkage = 0.1):\n",
    "        self.n_boo = n_boo\n",
    "        self.n_bag = n_bag\n",
    "        self.bagging_ratio = bagging_ratio\n",
    "        self.rsm_ratio = rsm_ratio\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.boosting_list = []\n",
    "        self.shrinkage = shrinkage\n",
    "        \n",
    "    def fit(self, X, Y, verbose = 0, X_test = None, Y_test = None):\n",
    "        cur_sum_predict = 0 \n",
    "        cur_sum_train = 0\n",
    "        error_statistic = []\n",
    "        rsm_cnt = int(self.rsm_ratio * X.shape[1])\n",
    "        bagging_cnt = int(self.bagging_ratio * X.shape[0])\n",
    "        print \"features in RMS: \", rsm_cnt\n",
    "        print \"samples in bagging: \", bagging_cnt\n",
    "        for bag_iter in xrange(self.n_bag):\n",
    "            sys.stderr.write('\\rIteration of bagging:'+str(bag_iter) + \"/\" + str(self.n_bag))\n",
    "#            print \"Iteration of bagging: \"+ str(bag_iter) + \"/\" + str(self.n_bag)\n",
    "            shuffle_idx_bagging = range(X.shape[0])\n",
    "            shuffle_idx_rsm = range(X.shape[1])\n",
    "            \n",
    "            r.shuffle(shuffle_idx_bagging)\n",
    "            r.shuffle(shuffle_idx_rsm)\n",
    "            shuffle_idx_bagging = shuffle_idx_bagging[:bagging_cnt]\n",
    "            shuffle_idx_rsm = shuffle_idx_rsm[:rsm_cnt]\n",
    "\n",
    "#             print shuffle_idx_bagging, shuffle_idx_rsm\n",
    "            X_bag = X[shuffle_idx_bagging][:, shuffle_idx_rsm]\n",
    "            Y_bag = Y[shuffle_idx_bagging]\n",
    "#             print \"sgs\", X_bag, Y_bag\n",
    "            \n",
    "            new_boosting = Gradient_Boosting(n_estimators= self.n_boo, max_depth=self.max_depth,\\\n",
    "                                             min_samples_leaf= self.min_samples_leaf, shrinkage = self.shrinkage)\n",
    "            new_boosting.fit(X_bag, Y_bag)\n",
    "            \n",
    "            self.boosting_list.append(new_boosting)\n",
    "            \n",
    "            if verbose:\n",
    "                cur_sum_predict += new_boosting.predict(X_test)\n",
    "                cur_sum_train += new_boosting.predict(X)\n",
    "                error_test =  MSE(cur_sum_predict / float(len(self.boosting_list)), Y_test)\n",
    "                error_train =  MSE(cur_sum_train / float(len(self.boosting_list)), Y)\n",
    "                print \"MSE on test Dataset:\", error_test, \"Iteration of Bagging:\", bag_iter, \"/\", self.n_bag\n",
    "                print \"MSE on train Dataset:\", error_train, \"Iteration of Bagging:\", bag_iter, \"/\", self.n_bag\n",
    "                error_statistic.append(error_test)\n",
    "               \n",
    "        return error_statistic    \n",
    "\n",
    "    def predict(self, X):\n",
    "        y = np.array([0.0] * X.shape[0])\n",
    "        for boosting in self.boosting_list:\n",
    "            y += boosting.predict(X)\n",
    "        return y / float(self.n_bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "f = [1, 2, 3]\n",
    "r.shuffle(f)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 5], [2, 3, 5], [2, 3, 4], [2, 9, 1], [2, 9, 4], [1, 2, 1]]*10)\n",
    "Y = np.array([1, 0, 3, 6, 4, 1]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features in RMS:  2\n",
      "samples in bagging:  3\n",
      "[1, 3, 0] [2, 1]\n",
      "sgs [[5 3]\n",
      " [1 9]\n",
      " [5 0]] [0 6 1]\n",
      "[3, 1, 2] [0, 1]\n",
      "sgs [[2 9]\n",
      " [2 3]\n",
      " [2 3]] [6 0 3]\n",
      "[0, 4, 5] [2, 1]\n",
      "sgs [[5 0]\n",
      " [4 9]\n",
      " [1 2]] [1 4 1]\n",
      "[0, 1, 4] [0, 2]\n",
      "sgs [[1 5]\n",
      " [2 5]\n",
      " [2 4]] [1 0 4]\n",
      "[5, 0, 2] [2, 0]\n",
      "sgs [[1 1]\n",
      " [5 1]\n",
      " [4 2]] [1 1 3]\n",
      "[0, 3, 4] [2, 0]\n",
      "sgs [[5 1]\n",
      " [1 2]\n",
      " [4 2]] [1 6 4]\n",
      "[4, 2, 3] [0, 2]\n",
      "sgs [[2 4]\n",
      " [2 4]\n",
      " [2 1]] [4 3 6]\n",
      "[3, 2, 1] [1, 2]\n",
      "sgs [[9 1]\n",
      " [3 4]\n",
      " [3 5]] [6 3 0]\n",
      "[5, 0, 2] [2, 0]\n",
      "sgs [[1 1]\n",
      " [5 1]\n",
      " [4 2]] [1 1 3]\n",
      "[2, 1, 0] [2, 1]\n",
      "sgs [[4 3]\n",
      " [5 3]\n",
      " [5 0]] [3 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration of bagging:0/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.142916666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.123679166667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.108096791667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.0954750679167\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.0852514716792\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.0769703587268\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.0702626572354\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.0648294190273\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.0604284960788\r",
      "Iteration of bagging:1/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 6.0\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 6.0\r",
      "Iteration of bagging:2/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 1.28625\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 1.1131125\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.972871125\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.85927561125\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.767263245112\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.692733228541\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.632363915118\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.583464771246\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.543856464709\r",
      "Iteration of bagging:3/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.142916666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.123679166667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.108096791667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.0954750679167\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.0852514716792\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.0769703587268\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.0702626572354\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.0648294190273\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.0604284960788\r",
      "Iteration of bagging:4/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.666666666667\r",
      "Iteration of bagging:5/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.571666666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.494716666667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.432387166667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.381900271667\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.341005886717\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.307881434907\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.281050628941\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.259317676109\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.241713984315\r",
      "Iteration of bagging:6/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.166666666667\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.166666666667\r",
      "Iteration of bagging:7/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 1.5\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 1.5\r",
      "Iteration of bagging:8/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.666666666667\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.666666666667\r",
      "Iteration of bagging:9/10\r",
      "Learning estimator number: 1/10; MSE error on train dataset: 0.142916666667\r",
      "Learning estimator number: 2/10; MSE error on train dataset: 0.123679166667\r",
      "Learning estimator number: 3/10; MSE error on train dataset: 0.108096791667\r",
      "Learning estimator number: 4/10; MSE error on train dataset: 0.0954750679167\r",
      "Learning estimator number: 5/10; MSE error on train dataset: 0.0852514716792\r",
      "Learning estimator number: 6/10; MSE error on train dataset: 0.0769703587268\r",
      "Learning estimator number: 7/10; MSE error on train dataset: 0.0702626572354\r",
      "Learning estimator number: 8/10; MSE error on train dataset: 0.0648294190273\r",
      "Learning estimator number: 9/10; MSE error on train dataset: 0.0604284960788"
     ]
    }
   ],
   "source": [
    "test = BagBoo(bagging_ratio=0.5, rsm_ratio=0.8).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6], [3, 4, 5]])\n",
    "print x[np.array([0,2])][:, np.array([1, 2])]\n",
    "# [: np.array([2, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_1 = Tree().fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25,  1.25])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_1.predict([[1, 2, 3],[8,-2,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 9?\n",
      "\t T->\n",
      "5.0\n",
      "\t F->\n",
      "1.25\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print tree_1.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avto-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n",
      "[[18.0 8 307.0 ..., 70 1 'chevrolet chevelle malibu']\n",
      " [15.0 8 350.0 ..., 70 1 'buick skylark 320']\n",
      " [18.0 8 318.0 ..., 70 1 'plymouth satellite']\n",
      " ..., \n",
      " [32.0 4 135.0 ..., 82 1 'dodge rampage']\n",
      " [28.0 4 120.0 ..., 82 1 'ford ranger']\n",
      " [31.0 4 119.0 ..., 82 1 'chevy s-10']]\n"
     ]
    }
   ],
   "source": [
    "avto = pd.read_csv(\"./data/auto-mpg.data.csv\", sep='\\t',header=None)\n",
    "avto = avto.values\n",
    "print avto.shape\n",
    "print avto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avto_data = avto[:, 1:8]\n",
    "avto_target = avto[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 307.0 130.0 ..., 12.0 70 1]\n",
      " [8 350.0 165.0 ..., 11.5 70 1]\n",
      " [8 318.0 150.0 ..., 11.0 70 1]\n",
      " ..., \n",
      " [4 135.0 84.0 ..., 11.6 82 1]\n",
      " [4 120.0 79.0 ..., 18.6 82 1]\n",
      " [4 119.0 82.0 ..., 19.4 82 1]]\n",
      "[18.0 15.0 18.0 16.0 17.0]\n"
     ]
    }
   ],
   "source": [
    "print avto_data\n",
    "print avto_target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 612 ms, sys: 4 ms, total: 616 ms\n",
      "Wall time: 685 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree_avto = Tree(depth=4).fit(avto_data[:350], avto_target[:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  24.0325897867\n"
     ]
    }
   ],
   "source": [
    "tree_avto.score(avto_data[350:392], avto_target[350:392])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4742424835\n"
     ]
    }
   ],
   "source": [
    "sk_avto = DecisionTreeRegressor(max_depth=4).fit(avto_data[:350], avto_target[:350])\n",
    "skl_predict = sk_avto.predict(avto_data[350:392])\n",
    "print ((skl_predict - avto_target[350:392])**2 / float(len(skl_predict))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 27/100; MSE error on train dataset: 3.85458861375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-deb3a5e3999e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'gb_avto = Gradient_Boosting(n_estimators=100, max_depth=2, shrinkage=0.1)    \\ngb_avto.fit(avto_data[:300], avto_target[:300])\\n\\nprint \"MSE Error on train dataset: \", MSE(gb_avto.predict(avto_data[:300]), avto_target[:300])\\nprint \"MSE Error on test dataset: \", MSE(gb_avto.predict(avto_data[300:]), avto_target[300:])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-779a1a7e6083>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mnew_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mnew_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantigrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;31m#             new_estimator.print_tree()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-779a1a7e6083>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_points_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_points_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_X_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_Y_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_X_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_Y_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_X_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_Y_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-779a1a7e6083>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m#             print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#             print \"Analusys of feature \", feat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_l\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_best_split_of_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mbest_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-779a1a7e6083>\u001b[0m in \u001b[0;36msearch_best_split_of_feat\u001b[0;34m(X, Y, split_feature, orig_var, min_samples_leaf)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mchecked_split_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         print np.where(X[:, split_feature] < split_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mX_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mX_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mY_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_feature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_avto = Gradient_Boosting(n_estimators=100, max_depth=2, shrinkage=0.1)    \n",
    "gb_avto.fit(avto_data[:300], avto_target[:300])\n",
    "\n",
    "print \"MSE Error on train dataset: \", MSE(gb_avto.predict(avto_data[:300]), avto_target[:300])\n",
    "print \"MSE Error on test dataset: \", MSE(gb_avto.predict(avto_data[300:]), avto_target[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Error on train dataset:  1.96854048843\n",
      "MSE Error on test dataset:  31.7097021516\n",
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 35.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skl_gb_avto = GradientBoostingRegressor(criterion='mse', min_impurity_split=0,\\\n",
    "                                        learning_rate=0.1, max_depth=2, n_estimators=100)\n",
    "skl_gb_avto.fit(avto_data[:300], avto_target[:300])\n",
    "print \"MSE Error on train dataset: \", MSE(skl_gb_avto.predict(avto_data[:300]), avto_target[:300])\n",
    "print \"MSE Error on test dataset: \", MSE(skl_gb_avto.predict(avto_data[300:]), avto_target[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7244551349\n"
     ]
    }
   ],
   "source": [
    "print ((skl_predict - avto_target[300:392])**2 / float(len(skl_predict))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 0/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.51237822924"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.57575090254"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.52627995698"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.94865160374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 3.57491181752"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.41203422296"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 2/20; MSE error on train dataset: 3.27860283389"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.47313202573"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.60934698372"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.62297715764"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.68431564783"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 2/20; MSE error on train dataset: 3.87455759653"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 4.22744527275"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.65297536705"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.36117454621"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.1529897468"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.84352090437"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.80226594516"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.42804108235"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.05907589768"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 3.39740630376"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 2.81164113273"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 2.82266586477"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 3.74057850677"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.57250852779"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.202396143"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.83563078814"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 2.61180005422"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.09548952157"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.65154925932"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.30208389194"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 1/20; MSE error on train dataset: 3.57247163271"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.60808277026"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.11124676966"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 2/20; MSE error on train dataset: 3.69292921738"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.31656808354"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.38772225412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.64054221837"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.39533344124"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.30888697765"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.43009700706"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.56487013878"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.27427611665"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 0.935906023048"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 2/20; MSE error on train dataset: 4.03596859345"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 2/20; MSE error on train dataset: 4.76152298867"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 2.80456661139"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.78940123739"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 19/20; MSE error on train dataset: 1.20035670834"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration of bagging: 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning estimator number: 17/20; MSE error on train dataset: 1.08085784396"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 1.17 s, total: 1min 24s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Learning estimator number: 18/20; MSE error on train dataset: 1.04492399702\r",
      "Learning estimator number: 19/20; MSE error on train dataset: 1.0078936038"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bagboo_avto = BagBoo(n_boo=20, n_bag=50, bagging_ratio=0.3, max_depth=3)    \n",
    "bagboo_avto.fit(avto_data[:300], avto_target[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16.42715978,  14.65455132,  15.62765702,  15.65619808,\n",
       "        16.47238784,  13.79927187,  13.72700477,  13.74249449,\n",
       "        13.52174017,  14.19534903])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagboo_avto.predict(avto_data[:300])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.23658112515\n",
      "37.1373884278\n"
     ]
    }
   ],
   "source": [
    "print MSE(bagboo_avto.predict(avto_data[:300]), avto_target[:300])\n",
    "print MSE(bagboo_avto.predict(avto_data[300:392]), avto_target[300:392])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00 ...,   3.96900000e+02\n",
      "    4.98000000e+00   2.40000000e+01]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00 ...,   3.96900000e+02\n",
      "    9.14000000e+00   2.16000000e+01]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00 ...,   3.92830000e+02\n",
      "    4.03000000e+00   3.47000000e+01]\n",
      " ..., \n",
      " [  6.07600000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02\n",
      "    5.64000000e+00   2.39000000e+01]\n",
      " [  1.09590000e-01   0.00000000e+00   1.19300000e+01 ...,   3.93450000e+02\n",
      "    6.48000000e+00   2.20000000e+01]\n",
      " [  4.74100000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02\n",
      "    7.88000000e+00   1.19000000e+01]]\n",
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00   0.00000000e+00\n",
      "    5.38000000e-01   6.57500000e+00   6.52000000e+01   4.09000000e+00\n",
      "    1.00000000e+00   2.96000000e+02   1.53000000e+01   3.96900000e+02\n",
      "    4.98000000e+00]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   6.42100000e+00   7.89000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.96900000e+02\n",
      "    9.14000000e+00]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00   0.00000000e+00\n",
      "    4.69000000e-01   7.18500000e+00   6.11000000e+01   4.96710000e+00\n",
      "    2.00000000e+00   2.42000000e+02   1.78000000e+01   3.92830000e+02\n",
      "    4.03000000e+00]\n",
      " [  3.23700000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   6.99800000e+00   4.58000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.94630000e+02\n",
      "    2.94000000e+00]\n",
      " [  6.90500000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   7.14700000e+00   5.42000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.96900000e+02\n",
      "    5.33000000e+00]\n",
      " [  2.98500000e-02   0.00000000e+00   2.18000000e+00   0.00000000e+00\n",
      "    4.58000000e-01   6.43000000e+00   5.87000000e+01   6.06220000e+00\n",
      "    3.00000000e+00   2.22000000e+02   1.87000000e+01   3.94120000e+02\n",
      "    5.21000000e+00]\n",
      " [  8.82900000e-02   1.25000000e+01   7.87000000e+00   0.00000000e+00\n",
      "    5.24000000e-01   6.01200000e+00   6.66000000e+01   5.56050000e+00\n",
      "    5.00000000e+00   3.11000000e+02   1.52000000e+01   3.95600000e+02\n",
      "    1.24300000e+01]\n",
      " [  1.44550000e-01   1.25000000e+01   7.87000000e+00   0.00000000e+00\n",
      "    5.24000000e-01   6.17200000e+00   9.61000000e+01   5.95050000e+00\n",
      "    5.00000000e+00   3.11000000e+02   1.52000000e+01   3.96900000e+02\n",
      "    1.91500000e+01]\n",
      " [  2.11240000e-01   1.25000000e+01   7.87000000e+00   0.00000000e+00\n",
      "    5.24000000e-01   5.63100000e+00   1.00000000e+02   6.08210000e+00\n",
      "    5.00000000e+00   3.11000000e+02   1.52000000e+01   3.86630000e+02\n",
      "    2.99300000e+01]\n",
      " [  1.70040000e-01   1.25000000e+01   7.87000000e+00   0.00000000e+00\n",
      "    5.24000000e-01   6.00400000e+00   8.59000000e+01   6.59210000e+00\n",
      "    5.00000000e+00   3.11000000e+02   1.52000000e+01   3.86710000e+02\n",
      "    1.71000000e+01]]\n",
      "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9]\n"
     ]
    }
   ],
   "source": [
    "house = pd.read_csv(\"./data/housing.data.csv\", sep='\\t',header=None)\n",
    "house = house.values\n",
    "print house.shape\n",
    "print house\n",
    "house_data = house[:, :13]\n",
    "house_target = house[:, 13]\n",
    "print house_data[:10]\n",
    "print house_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  55.02303951\n",
      "CPU times: user 552 ms, sys: 0 ns, total: 552 ms\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree_house = Tree(depth=4).fit(house_data[:300], house_target[:300])\n",
    "tree_house.score(house_data[300:392], house_target[300:392])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.02303951\n"
     ]
    }
   ],
   "source": [
    "sk_house = DecisionTreeRegressor(max_depth=4).fit(house_data[:300], house_target[:300])\n",
    "skl_predict = sk_house.predict(house_data[300:392])\n",
    "print ((skl_predict - house_target[300:392])**2 / float(len(skl_predict))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learning estimator: 0; MSE error on train dataset:  2558.21747067\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  1744.14453063\n",
      "Number of learning estimator:  20 ; MSE error on train dataset:  1308.94369915\n",
      "Number of learning estimator:  30 ; MSE error on train dataset:  1037.5467406\n",
      "Number of learning estimator:  40 ; MSE error on train dataset:  848.176763415\n",
      "Number of learning estimator:  50 ; MSE error on train dataset:  718.57484164\n",
      "Number of learning estimator:  60 ; MSE error on train dataset:  615.749565531\n",
      "Number of learning estimator:  70 ; MSE error on train dataset:  528.983190297\n",
      "Number of learning estimator:  80 ; MSE error on train dataset:  473.463570747\n",
      "Number of learning estimator:  90 ; MSE error on train dataset:  423.31122444\n",
      "Number of learning estimator:  100 ; MSE error on train dataset:  383.96837126\n",
      "MSE Error on train dataset:  383.96837126\n",
      "MSE Error on test dataset:  2019.41281341\n",
      "CPU times: user 55.3 s, sys: 48 ms, total: 55.4 s\n",
      "Wall time: 55.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_house = Gradient_Boosting(n_estimators=101, max_depth=3, shrinkage=0.1)    \n",
    "gb_house.fit(house_data[:400], house_target[:400])\n",
    "\n",
    "print \"MSE Error on train dataset: \", MSE(gb_house.predict(house_data[:400]), house_target[:400])\n",
    "print \"MSE Error on test dataset: \", MSE(gb_house.predict(house_data[400:506]), house_target[400:506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Error on train dataset:  320.259854388\n",
      "MSE Error on test dataset:  671.613810248\n",
      "CPU times: user 104 ms, sys: 0 ns, total: 104 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skl_gb_house = GradientBoostingRegressor(criterion='mse', min_impurity_split=0)\n",
    "skl_gb_house.fit(house_data[:400], house_target[:400])\n",
    "print \"MSE Error on train dataset: \", MSE(skl_gb_house.predict(house_data[:400]), house_target[:400])\n",
    "print \"MSE Error on test dataset: \", MSE( skl_gb_house.predict(house_data[400:506]), house_target[400:506])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computer Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 10)\n",
      "[['adviser' '32/60' 125 ..., 128 198 199]\n",
      " ['amdahl' '470v/7' 29 ..., 32 269 253]\n",
      " ['amdahl' '470v/7a' 29 ..., 32 220 253]\n",
      " ..., \n",
      " ['sratus' '32' 125 ..., 14 52 41]\n",
      " ['wang' 'vs-100' 480 ..., 0 67 47]\n",
      " ['wang' 'vs-90' 480 ..., 0 45 25]]\n",
      "[[125 256 6000 256 16 128 198]\n",
      " [29 8000 32000 32 8 32 269]\n",
      " [29 8000 32000 32 8 32 220]\n",
      " [29 8000 32000 32 8 32 172]\n",
      " [29 8000 16000 32 8 16 132]\n",
      " [26 8000 32000 64 8 32 318]\n",
      " [23 16000 32000 64 16 32 367]\n",
      " [23 16000 32000 64 16 32 489]\n",
      " [23 16000 64000 64 16 32 636]\n",
      " [23 32000 64000 128 32 64 1144]]\n",
      "[199 253 253 253 132 290 381 381 749 1238]\n"
     ]
    }
   ],
   "source": [
    "machine = pd.read_csv(\"./data/machine.data.csv\", sep=',',header=None)\n",
    "machine = machine.values\n",
    "print machine.shape\n",
    "print machine\n",
    "machine_data = machine[:, 2:9]\n",
    "machine_target = machine[:, 9]\n",
    "print machine_data[:10]\n",
    "print machine_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POINTS:  149\n",
      "N_POINTS:  1\n",
      "CPU times: user 172 ms, sys: 12 ms, total: 184 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tr_house = Tree().fit(machine_data[:150], machine_target[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_POINTS:  16\n",
      "N_POINTS:  12\n",
      "N_POINTS:  1\n",
      "N_POINTS:  1\n",
      "Number of learning estimator: 0\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  1\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  2\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  3\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  4\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  5\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  6\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  7\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  8\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "Number of learning estimator:  9\n",
      "N_POINTS:  30\n",
      "0.0\n",
      "set([-7.75, 46.25, 4.6875, 83.25, 33.6875, 8.6875, 14.6875, -15.3125, 0.0, 174.25, 9.6875, -7.3125, -6.3125, -136.75, -3.3125, -8.3125, -1.3125, -2.3125, -178.75, -82.75, -89.75, -9.3125, -74.75]) [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "MSE error on train dataset:  73723.84375\n",
      "CPU times: user 128 ms, sys: 4 ms, total: 132 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_tr = Gradient_Boosting(max_depth=3)    \n",
    "gb_tr.fit(machine_data[:30], machine_target[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7093, 103)\n",
      "(10056, 103)\n"
     ]
    }
   ],
   "source": [
    "spam_train = np.loadtxt(\"./data/spam.train.txt\")\n",
    "spam_test = np.loadtxt(\"./data/spam.test.txt\")\n",
    "spam_train_data = spam_train[0::, 1::]\n",
    "spam_train_target = spam_train[0::, 0]\n",
    "spam_test_data = spam_test[0::, 1::]\n",
    "spam_test_target = spam_test[0::, 0]\n",
    "print spam_train.shape\n",
    "print spam_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2970\n",
      "2033\n"
     ]
    }
   ],
   "source": [
    "# print spam_test_data[:1]\n",
    "# print spam_test_target[:455]\n",
    "# print spam_train_data[:1]\n",
    "# print spam_train_target[:455]\n",
    "print np.sum([1 for s in spam_train_target if s == 0])\n",
    "print np.sum([1 for s in spam_test_target if s == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-51910ef4483c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'gb_spam = Gradient_Boosting(n_estimators=10, max_depth=3, shrinkage=0.1)    \\ngb_spam.fit(spam_train_data, spam_train_target,verbose = 2)\\n# print \"MSE Error on train dataset: \", MSE(gb_spam.predict(house_data[:400]), house_target[:400])\\nprint \"MSE Error on test dataset: \", MSE(gb_spam.predict(spam_test_data), spam_test_target)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dana_zl/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_spam = Gradient_Boosting(n_estimators=10, max_depth=3, shrinkage=0.1)    \n",
    "gb_spam.fit(spam_train_data, spam_train_target)\n",
    "# print \"MSE Error on train dataset: \", MSE(gb_spam.predict(house_data[:400]), house_target[:400])\n",
    "print \"MSE Error on test dataset: \", MSE(gb_spam.predict(spam_test_data), spam_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0819392272764\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0475617430457\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0851488004344\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.049674908376\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0808008154593\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0456885511967\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.105370596725\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0589344066274\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0832367290717\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0517909094027\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0785564450533\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0488745509803\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0629150237617\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0338602593513\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0886439168123\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0448763932497\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0746988343599\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0440625019172\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0931596399122\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0550051210723\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0676943117673\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0410435323643\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0643555629522\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.037661663949\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0746862740114\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0447918210389\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0978898874756\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0599146827989\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.106018288215\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0642685832911\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0904548048348\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.063427574612\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0633920855424\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0407635323514\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0509711096549\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0254631624047\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0730653034908\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0403442302824\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0467728458466\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0284896883588\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0915665719448\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.049629281602\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.104491725768\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0583807996637\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0749888306009\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0445060977321\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.082427977101\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0455160482392\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.100713728489\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0538507037273\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0997596201601\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0568758694414\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.105632389827\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0572991208107\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0657837603126\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.032120826883\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0732214315244\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0455426485275\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0796905222437\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0478092054636\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0883853523057\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.053760348481\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.067737596328\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.039037958409\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0963330954399\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.054936153358\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0860484544695\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0485104794072\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0577663952368\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0326759060316\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0933813787381\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0560007467434\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0892585087621\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0538275446816\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0896661800917\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0519150627599\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.082473963325\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0459806194623\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0941446116589\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0569325893632\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0964078563967\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0578274563564\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.103998956127\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0671591890119\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0973229550873\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0507170439925\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0690698781124\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0389816246603\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0567921440262\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0374696073304\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0815810016389\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0463587718056\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0704014091863\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0426683230417\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0884151503796\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0534010237686\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0860678402567\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0544632451661\n",
      "7093\n",
      "Number of learning estimator: 0; MSE error on train dataset:  0.0745687856948\n",
      "Number of learning estimator:  10 ; MSE error on train dataset:  0.0424898855429\n",
      "MSE Error on test dataset:  0.24906008556\n",
      "CPU times: user 12min 8s, sys: 292 ms, total: 12min 9s\n",
      "Wall time: 12min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bagboo_spam = BagBoo(n_boo=20, n_bag=50, bagging_ratio=0.02, max_depth=2)    \n",
    "bagboo_spam.fit(spam_train_data, spam_train_target)\n",
    "\n",
    "print \"MSE Error on test dataset: \", MSE(bagboo_spam.predict(spam_test_data), spam_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2404            8.65s\n",
      "         2           0.2375            8.86s\n",
      "         3           0.2347            8.38s\n",
      "         4           0.2319            8.14s\n",
      "         5           0.2291            7.62s\n",
      "         6           0.2264            7.27s\n",
      "         7           0.2238            7.20s\n",
      "         8           0.2212            6.87s\n",
      "         9           0.2186            6.49s\n",
      "        10           0.2161            6.32s\n",
      "        11           0.2137            6.13s\n",
      "        12           0.2112            5.87s\n",
      "        13           0.2089            5.66s\n",
      "        14           0.2066            5.46s\n",
      "        15           0.2043            5.43s\n",
      "        16           0.2020            5.29s\n",
      "        17           0.1999            5.24s\n",
      "        18           0.1977            5.19s\n",
      "        19           0.1956            5.24s\n",
      "        20           0.1935            5.21s\n",
      "        21           0.1914            5.16s\n",
      "        22           0.1894            5.12s\n",
      "        23           0.1875            5.10s\n",
      "        24           0.1855            5.05s\n",
      "        25           0.1836            5.00s\n",
      "        26           0.1818            4.96s\n",
      "        27           0.1799            4.90s\n",
      "        28           0.1781            4.85s\n",
      "        29           0.1764            4.82s\n",
      "        30           0.1746            4.75s\n",
      "        31           0.1729            4.69s\n",
      "        32           0.1712            4.64s\n",
      "        33           0.1696            4.57s\n",
      "        34           0.1680            4.50s\n",
      "        35           0.1664            4.44s\n",
      "        36           0.1648            4.37s\n",
      "        37           0.1631            4.30s\n",
      "        38           0.1616            4.25s\n",
      "        39           0.1601            4.17s\n",
      "        40           0.1585            4.11s\n",
      "        41           0.1569            4.05s\n",
      "        42           0.1554            3.99s\n",
      "        43           0.1540            3.93s\n",
      "        44           0.1525            3.89s\n",
      "        45           0.1510            3.82s\n",
      "        46           0.1496            3.76s\n",
      "        47           0.1482            3.70s\n",
      "        48           0.1469            3.63s\n",
      "        49           0.1455            3.58s\n",
      "        50           0.1442            3.54s\n",
      "        51           0.1429            3.49s\n",
      "        52           0.1416            3.43s\n",
      "        53           0.1404            3.37s\n",
      "        54           0.1392            3.31s\n",
      "        55           0.1380            3.25s\n",
      "        56           0.1368            3.20s\n",
      "        57           0.1357            3.12s\n",
      "        58           0.1345            3.06s\n",
      "        59           0.1334            2.99s\n",
      "        60           0.1323            2.92s\n",
      "        61           0.1312            2.86s\n",
      "        62           0.1299            2.79s\n",
      "        63           0.1288            2.72s\n",
      "        64           0.1276            2.66s\n",
      "        65           0.1265            2.60s\n",
      "        66           0.1254            2.53s\n",
      "        67           0.1244            2.46s\n",
      "        68           0.1233            2.41s\n",
      "        69           0.1224            2.34s\n",
      "        70           0.1213            2.27s\n",
      "        71           0.1203            2.21s\n",
      "        72           0.1194            2.14s\n",
      "        73           0.1184            2.07s\n",
      "        74           0.1174            2.00s\n",
      "        75           0.1165            1.92s\n",
      "        76           0.1156            1.85s\n",
      "        77           0.1147            1.78s\n",
      "        78           0.1138            1.71s\n",
      "        79           0.1130            1.63s\n",
      "        80           0.1121            1.56s\n",
      "        81           0.1113            1.49s\n",
      "        82           0.1104            1.41s\n",
      "        83           0.1096            1.34s\n",
      "        84           0.1087            1.26s\n",
      "        85           0.1078            1.18s\n",
      "        86           0.1070            1.11s\n",
      "        87           0.1062            1.03s\n",
      "        88           0.1054            0.96s\n",
      "        89           0.1045            0.88s\n",
      "        90           0.1038            0.80s\n",
      "        91           0.1030            0.72s\n",
      "        92           0.1022            0.64s\n",
      "        93           0.1015            0.56s\n",
      "        94           0.1007            0.48s\n",
      "        95           0.1000            0.40s\n",
      "        96           0.0993            0.32s\n",
      "        97           0.0986            0.24s\n",
      "        98           0.0978            0.16s\n",
      "        99           0.0970            0.08s\n",
      "       100           0.0964            0.00s\n",
      "MSE Error on test dataset:  0.0639735180761\n",
      "CPU times: user 7.9 s, sys: 12 ms, total: 7.91 s\n",
      "Wall time: 8.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "skl_gb_spam = GradientBoostingRegressor(criterion='mse', min_impurity_split=0, verbose=2, learning_rate=0.01)\n",
    "skl_gb_spam.fit(spam_train_data, spam_train_target)\n",
    "# print \"MSE Error on train dataset: \", MSE(skl_gb_house.predict(house_data[:400]), house_target[:400])\n",
    "print \"MSE Error on test dataset: \", MSE( skl_gb_house.predict(spam_test_data), spam_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
